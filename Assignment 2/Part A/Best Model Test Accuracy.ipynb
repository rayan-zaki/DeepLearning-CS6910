{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "passive-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration cell\n",
    "\n",
    "no_epochs = 30\n",
    "\n",
    "act_fn_dict = {\n",
    "    'conv1':'relu',\n",
    "    'conv2':'relu',\n",
    "    'conv3':'relu',\n",
    "    'conv4':'relu',\n",
    "    'conv5':'relu',\n",
    "    'fc1':'relu'\n",
    "}\n",
    "\n",
    "kernel_size_list = [3,3,3,3,3]\n",
    "no_kernel_list = [32,32,64,64,128]\n",
    "dropout_list = [0,0,0.5]\n",
    "fc1_nodes = 1024\n",
    "no_classes = 10\n",
    "lr = 0.0001\n",
    "lr_schedule = 0.5 # per 10 epochs half the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import wandb\n",
    "from util import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "civic-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda == True:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "immediate-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'Best_CNN_5Layers_iNaturalist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "national-cosmetic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already present\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(modelName+\"/checkpoint\")\n",
    "    os.makedirs(modelName+\"/best_model\")\n",
    "except:\n",
    "    print(\"directory already present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dental-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_path = \"./\"+modelName+\"/checkpoint/current_checkpoint.pt\"\n",
    "best_ckp_path = \"./\"+modelName+\"/best_model/best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stainless-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: rayanz (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.10.26 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fancy-puddle-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/rayanz/Best_CNN_5Layers_iNaturalist\" target=\"_blank\">https://wandb.ai/rayanz/Best_CNN_5Layers_iNaturalist</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/rayanz/Best_CNN_5Layers_iNaturalist/runs/fcnlulcs\" target=\"_blank\">https://wandb.ai/rayanz/Best_CNN_5Layers_iNaturalist/runs/fcnlulcs</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\Rayan Zaki\\Desktop\\Fundamentals of Deep Learning\\Assignment 2\\Part A\\wandb\\run-20210416_112452-fcnlulcs</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(fcnlulcs)</h1><iframe src=\"https://wandb.ai/rayanz/Best_CNN_5Layers_iNaturalist/runs/fcnlulcs\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x204fb3d1a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "experienced-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "datasetTrain, datasetVal, datasetTest = load_datasets()\n",
    "loaders = data_loader(datasetTrain, datasetVal, datasetTest, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "iraqi-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_fn(act_name):\n",
    "    if act_name==\"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    \n",
    "    elif act_name==\"sigmoid\":\n",
    "        return nn.Sigmoid(inplace=True)\n",
    "    \n",
    "    elif act_name==\"tanh\":\n",
    "        return nn.Tanh(inplace==True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "physical-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 3 ,BN=True , NL=\"relu\", stride = 1, padding = 0):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.BN=BN\n",
    "        self.NL=NL\n",
    "        k = kernel_size\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = k, stride = stride, padding = padding, bias=False)\n",
    "        \n",
    "        if self.BN==True:\n",
    "            self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "            \n",
    "        self.act = act_fn(NL)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.BN==True:\n",
    "            x = self.bn(x)\n",
    "        \n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "intense-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, BN=False , NL=\"relu\"):\n",
    "        super(fc_block, self).__init__()\n",
    "        self.BN=BN\n",
    "        self.NL=NL\n",
    "        self.fc = nn.Linear(in_channels, out_channels)\n",
    "        \n",
    "        if self.BN==True:\n",
    "            self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "            \n",
    "        self.act = act_fn(NL)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if self.BN==True:\n",
    "            x = self.bn(x)\n",
    "        \n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ethical-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc_in(input_dim, kernel_size_list, no_kernel_list):\n",
    "    H = input_dim\n",
    "    fc_in = H - kernel_size_list[0] + 1 # conv1\n",
    "    fc_in = (fc_in - 2) //2  + 1 # max pool 1\n",
    "    fc_in = fc_in - kernel_size_list[1] + 1 # conv2\n",
    "    fc_in = (fc_in - 2) //2  + 1 # max pool 2\n",
    "    fc_in = fc_in - kernel_size_list[2] + 1 #conv3\n",
    "    fc_in = (fc_in - 2) //2  + 1 # max pool 3\n",
    "    fc_in = fc_in - kernel_size_list[3] + 1 #conv4\n",
    "    fc_in = (fc_in - 2) //2  + 1 # max pool 4\n",
    "    fc_in = fc_in - kernel_size_list[4] + 1 #conv5\n",
    "    fc_in = (fc_in - 2) //2  + 1 # max pool 5\n",
    "    #print(fc_in)\n",
    "    return fc_in * fc_in * no_kernel_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unique-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_5layer(nn.Module):\n",
    "    def __init__(self, kernel_size_list, no_kernel_list, act_fn_dict, dropout_list, fc1_nodes, no_classes):\n",
    "        super(CNN_5layer, self).__init__()\n",
    "        self.dropout_list = dropout_list\n",
    "        self.input_dim = 224\n",
    "        #self.input_dim = 128\n",
    "        self.conv1 = conv_block(3, no_kernel_list[0], kernel_size=kernel_size_list[0], BN=False, NL=act_fn_dict['conv1'])\n",
    "        self.maxpool1 = nn.MaxPool2d((2, 2))\n",
    "        self.conv2 = conv_block(no_kernel_list[0], no_kernel_list[1], kernel_size=kernel_size_list[1], BN=True, NL=act_fn_dict['conv2'])\n",
    "        self.maxpool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        if self.dropout_list[0]!=0:\n",
    "            self.dropout1 = nn.Dropout(dropout_list[0])\n",
    "\n",
    "        self.conv3 = conv_block(no_kernel_list[1], no_kernel_list[2], kernel_size=kernel_size_list[2], BN=True, NL=act_fn_dict['conv3'])\n",
    "        self.maxpool3 = nn.MaxPool2d((2, 2))\n",
    "        self.conv4 = conv_block(no_kernel_list[2], no_kernel_list[3], kernel_size=kernel_size_list[3], BN=True, NL=act_fn_dict['conv4'])\n",
    "        self.maxpool4 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        if self.dropout_list[1]!=0:\n",
    "            self.dropout2 = nn.Dropout(dropout_list[1])\n",
    "\n",
    "        self.conv5 = conv_block(no_kernel_list[3], no_kernel_list[4], kernel_size=kernel_size_list[4], BN=True, NL=act_fn_dict['conv5'])\n",
    "        self.maxpool5 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.fc1_in_features = get_fc_in(self.input_dim, kernel_size_list, no_kernel_list)\n",
    "        \n",
    "        self.fc1 = fc_block(self.fc1_in_features, fc1_nodes , NL=act_fn_dict['fc1'])\n",
    "        \n",
    "        if self.dropout_list[2]!=0:\n",
    "            self.dropout3 = nn.Dropout(dropout_list[2])\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc1_nodes, no_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.shape[2]!=self.input_dim:\n",
    "            print(\"input dim not matched\")\n",
    "            return\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        if self.dropout_list[0]!=0:\n",
    "            x = self.dropout1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        if self.dropout_list[1]!=0:\n",
    "            x = self.dropout2(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        if self.dropout_list[2]!=0:\n",
    "            x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "inclusive-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_5layer(kernel_size_list, no_kernel_list, act_fn_dict, dropout_list, fc1_nodes, no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "criminal-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eleven-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=lr_schedule)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "experimental-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(start_epochs, n_epochs, valid_loss_min_input, loaders, model, optimizer, criterion,scheduler, use_cuda, checkpoint_path, best_model_path):\n",
    "    \n",
    "    valid_loss_min = valid_loss_min_input \n",
    "    \n",
    "    for epoch in range(start_epochs, start_epochs+n_epochs):\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        tnum_correct = 0\n",
    "        tnum_examples = 0\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1],target).view(-1)\n",
    "            tnum_correct += torch.sum(correct).item()\n",
    "            tnum_examples += correct.shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        train_acc = tnum_correct / tnum_examples\n",
    "        \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            \n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1],target).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "            \n",
    "        train_loss = train_loss/len(loaders['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders['valid'].dataset)\n",
    "        valid_acc = num_correct / num_examples\n",
    "        scheduler.step()\n",
    "        \n",
    "        print('Epoch: {}\\tTraining Loss: {:.6f}\\tTrain Accuracy: {:.2f}\\tValidation Loss: {:.6f}\\tvalidation Accuracy: {:.2f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            train_acc,\n",
    "            valid_loss,\n",
    "            valid_acc\n",
    "            ))\n",
    "        \n",
    "        wandb.log({'epoch': epoch,'train loss': train_loss,'train accuracy': train_acc,\n",
    "                   'val loss': valid_loss, 'val accuracy': valid_acc})\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'valid_loss': valid_loss,\n",
    "            'valid_acc': valid_acc,\n",
    "            'valid_loss_min': valid_loss_min,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        \n",
    "        save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "        \n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "            # save checkpoint as best model\n",
    "            save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confirmed-healthcare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTraining Loss: 0.000242\tTrain Accuracy: 0.21\tValidation Loss: 0.002002\tvalidation Accuracy: 0.30\n",
      "Validation loss decreased (inf --> 0.002002).  Saving model ...\n",
      "Epoch: 2\tTraining Loss: 0.000231\tTrain Accuracy: 0.25\tValidation Loss: 0.001939\tvalidation Accuracy: 0.32\n",
      "Validation loss decreased (0.002002 --> 0.001939).  Saving model ...\n",
      "Epoch: 3\tTraining Loss: 0.000226\tTrain Accuracy: 0.27\tValidation Loss: 0.001886\tvalidation Accuracy: 0.33\n",
      "Validation loss decreased (0.001939 --> 0.001886).  Saving model ...\n",
      "Epoch: 4\tTraining Loss: 0.000223\tTrain Accuracy: 0.28\tValidation Loss: 0.001879\tvalidation Accuracy: 0.36\n",
      "Validation loss decreased (0.001886 --> 0.001879).  Saving model ...\n",
      "Epoch: 5\tTraining Loss: 0.000219\tTrain Accuracy: 0.30\tValidation Loss: 0.001857\tvalidation Accuracy: 0.35\n",
      "Validation loss decreased (0.001879 --> 0.001857).  Saving model ...\n",
      "Epoch: 6\tTraining Loss: 0.000217\tTrain Accuracy: 0.30\tValidation Loss: 0.001848\tvalidation Accuracy: 0.34\n",
      "Validation loss decreased (0.001857 --> 0.001848).  Saving model ...\n",
      "Epoch: 7\tTraining Loss: 0.000215\tTrain Accuracy: 0.31\tValidation Loss: 0.001801\tvalidation Accuracy: 0.38\n",
      "Validation loss decreased (0.001848 --> 0.001801).  Saving model ...\n",
      "Epoch: 8\tTraining Loss: 0.000214\tTrain Accuracy: 0.32\tValidation Loss: 0.001798\tvalidation Accuracy: 0.38\n",
      "Validation loss decreased (0.001801 --> 0.001798).  Saving model ...\n",
      "Epoch: 9\tTraining Loss: 0.000210\tTrain Accuracy: 0.34\tValidation Loss: 0.001794\tvalidation Accuracy: 0.36\n",
      "Validation loss decreased (0.001798 --> 0.001794).  Saving model ...\n",
      "Epoch: 10\tTraining Loss: 0.000211\tTrain Accuracy: 0.32\tValidation Loss: 0.001758\tvalidation Accuracy: 0.39\n",
      "Validation loss decreased (0.001794 --> 0.001758).  Saving model ...\n",
      "Epoch: 11\tTraining Loss: 0.000204\tTrain Accuracy: 0.35\tValidation Loss: 0.001768\tvalidation Accuracy: 0.38\n",
      "Epoch: 12\tTraining Loss: 0.000203\tTrain Accuracy: 0.36\tValidation Loss: 0.001704\tvalidation Accuracy: 0.40\n",
      "Validation loss decreased (0.001758 --> 0.001704).  Saving model ...\n",
      "Epoch: 13\tTraining Loss: 0.000202\tTrain Accuracy: 0.36\tValidation Loss: 0.001698\tvalidation Accuracy: 0.42\n",
      "Validation loss decreased (0.001704 --> 0.001698).  Saving model ...\n",
      "Epoch: 14\tTraining Loss: 0.000200\tTrain Accuracy: 0.36\tValidation Loss: 0.001687\tvalidation Accuracy: 0.43\n",
      "Validation loss decreased (0.001698 --> 0.001687).  Saving model ...\n",
      "Epoch: 15\tTraining Loss: 0.000199\tTrain Accuracy: 0.36\tValidation Loss: 0.001684\tvalidation Accuracy: 0.40\n",
      "Validation loss decreased (0.001687 --> 0.001684).  Saving model ...\n",
      "Epoch: 16\tTraining Loss: 0.000200\tTrain Accuracy: 0.36\tValidation Loss: 0.001678\tvalidation Accuracy: 0.43\n",
      "Validation loss decreased (0.001684 --> 0.001678).  Saving model ...\n",
      "Epoch: 17\tTraining Loss: 0.000198\tTrain Accuracy: 0.37\tValidation Loss: 0.001671\tvalidation Accuracy: 0.42\n",
      "Validation loss decreased (0.001678 --> 0.001671).  Saving model ...\n",
      "Epoch: 18\tTraining Loss: 0.000197\tTrain Accuracy: 0.37\tValidation Loss: 0.001676\tvalidation Accuracy: 0.43\n",
      "Epoch: 19\tTraining Loss: 0.000197\tTrain Accuracy: 0.38\tValidation Loss: 0.001631\tvalidation Accuracy: 0.44\n",
      "Validation loss decreased (0.001671 --> 0.001631).  Saving model ...\n",
      "Epoch: 20\tTraining Loss: 0.000195\tTrain Accuracy: 0.39\tValidation Loss: 0.001637\tvalidation Accuracy: 0.45\n",
      "Epoch: 21\tTraining Loss: 0.000193\tTrain Accuracy: 0.39\tValidation Loss: 0.001635\tvalidation Accuracy: 0.45\n",
      "Epoch: 22\tTraining Loss: 0.000193\tTrain Accuracy: 0.39\tValidation Loss: 0.001620\tvalidation Accuracy: 0.45\n",
      "Validation loss decreased (0.001631 --> 0.001620).  Saving model ...\n",
      "Epoch: 23\tTraining Loss: 0.000191\tTrain Accuracy: 0.40\tValidation Loss: 0.001618\tvalidation Accuracy: 0.43\n",
      "Validation loss decreased (0.001620 --> 0.001618).  Saving model ...\n",
      "Epoch: 24\tTraining Loss: 0.000191\tTrain Accuracy: 0.40\tValidation Loss: 0.001598\tvalidation Accuracy: 0.44\n",
      "Validation loss decreased (0.001618 --> 0.001598).  Saving model ...\n",
      "Epoch: 25\tTraining Loss: 0.000192\tTrain Accuracy: 0.39\tValidation Loss: 0.001597\tvalidation Accuracy: 0.45\n",
      "Validation loss decreased (0.001598 --> 0.001597).  Saving model ...\n",
      "Epoch: 26\tTraining Loss: 0.000190\tTrain Accuracy: 0.40\tValidation Loss: 0.001607\tvalidation Accuracy: 0.45\n",
      "Epoch: 27\tTraining Loss: 0.000189\tTrain Accuracy: 0.40\tValidation Loss: 0.001601\tvalidation Accuracy: 0.45\n",
      "Epoch: 28\tTraining Loss: 0.000189\tTrain Accuracy: 0.40\tValidation Loss: 0.001597\tvalidation Accuracy: 0.43\n",
      "Epoch: 29\tTraining Loss: 0.000188\tTrain Accuracy: 0.41\tValidation Loss: 0.001580\tvalidation Accuracy: 0.45\n",
      "Validation loss decreased (0.001597 --> 0.001580).  Saving model ...\n",
      "Epoch: 30\tTraining Loss: 0.000189\tTrain Accuracy: 0.40\tValidation Loss: 0.001592\tvalidation Accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(start_epochs = 1,\n",
    "                      n_epochs = no_epochs,\n",
    "                      valid_loss_min_input = np.Inf,\n",
    "                      loaders = loaders,\n",
    "                      model = model,\n",
    "                      optimizer = optimizer,\n",
    "                      criterion = criterion,\n",
    "                      scheduler = scheduler,\n",
    "                      use_cuda = use_cuda,\n",
    "                      checkpoint_path = ckp_path,\n",
    "                      best_model_path = best_ckp_path\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-porcelain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "primary-boundary",
   "metadata": {},
   "source": [
    "## Training accuracy for best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rental-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved last checkpoint\n",
    "best_trained_model, optimizer, start_epoch, valid_loss, valid_acc, valid_loss_min = load_ckp(best_ckp_path, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "assumed-productivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model is : 42%\n"
     ]
    }
   ],
   "source": [
    "best_trained_model.eval()\n",
    "test_acc = 0.0\n",
    "test_num_correct = 0\n",
    "test_num_examples = 0\n",
    "for data, target in loaders['test']:\n",
    "    with torch.no_grad():\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output = best_trained_model(data)\n",
    "        # calculate accuracy\n",
    "        correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1],target).view(-1)\n",
    "        test_num_correct += torch.sum(correct).item()\n",
    "        test_num_examples += correct.shape[0]\n",
    "        \n",
    "test_acc = test_num_correct / test_num_examples\n",
    "print('Test Accuracy of the model is : {}%'.format(round(test_acc*100.0), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "saved-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"Test Accuracy\": test_acc})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
